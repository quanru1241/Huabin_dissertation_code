{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9b9fcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ÂΩìÂâçÂ∑•‰ΩúÁõÆÂΩï: /root/autodl-tmp/dataset\n",
      "ÊÄªËÆ°ÂåπÈÖçÂà∞ 170 ‰∏™ Met Êñá‰ª∂\n",
      "\n",
      "================================================================================\n",
      "üöÄ Ê≠£Âú®Â§ÑÁêÜÂâç 5 ‰∏™Á´ôÁÇπ ...\n",
      "‚úÖ Êï∞ÊçÆÂáÜÂ§áÂÆåÊàêÔºöÁ´ôÁÇπÊï∞ 5ÔºåÊÄªËÆ∞ÂΩï 561019\n",
      "üß™ Split done: train=504916, test=56103 (by-site tail 10%)\n",
      "‚öôÔ∏è  FLAML fit: target=NEE, sites=5, budget=240s, estimators=['xgboost', 'lgbm', 'rf', 'extra_tree', 'lrl1']\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "import gc\n",
    "import sys\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "\n",
    "from flaml import AutoML\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from scipy.stats import pearsonr\n",
    "import joblib\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ========================== 1) Global Parameters ==========================\n",
    "met_folder = \"plumber2_met_nc_files\"\n",
    "flux_folder = \"plumber2_nc_files\"\n",
    "\n",
    "features_raw = ['SWdown', 'LWdown', 'Tair', 'Qair', 'RH', 'Psurf', 'Wind',\n",
    "                'CO2air', 'VPD', 'LAI', 'Ustar']\n",
    "derived_features = ['SW_LAI', 'RH_Tair', 'SWdown_lag1', 'Tair_lag1']\n",
    "geo_features = ['latitude', 'longitude']\n",
    "features_all = features_raw + derived_features + geo_features\n",
    "\n",
    "# Batch experiments: 5/10/20/50/170\n",
    "site_limits = [5, 10, 20, 50, 170]\n",
    "\n",
    "# Time budget (seconds) for each scale\n",
    "time_budget_map = {5: 240, 10: 420, 20: 900, 50: 1500, 170: 2400}\n",
    "default_time_budget = 600\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "N_SPLITS = 5\n",
    "TEST_FRAC_PER_SITE = 0.10  # The last 10% of each site reserved as hold-out test set\n",
    "\n",
    "# ========================== 2) xarray open function compatible with Windows non-ASCII paths ==========================\n",
    "_def_open_backends = (\"netcdf4\", \"h5netcdf\")\n",
    "\n",
    "def _has_non_ascii(s: str) -> bool:\n",
    "    try:\n",
    "        s.encode('ascii')\n",
    "        return False\n",
    "    except UnicodeEncodeError:\n",
    "        return True\n",
    "\n",
    "def _ascii_cache_path(src_path: str) -> str:\n",
    "    cache_root = Path(os.getenv('TEMP', 'C:\\\\temp')) / 'nc_cache'\n",
    "    cache_root.mkdir(parents=True, exist_ok=True)\n",
    "    return str(cache_root / Path(src_path).name)\n",
    "\n",
    "def xr_open(path):\n",
    "    abs_path = str(Path(path).resolve())\n",
    "    if sys.platform.startswith('win') and _has_non_ascii(abs_path):\n",
    "        cached = _ascii_cache_path(abs_path)\n",
    "        try:\n",
    "            if (not os.path.exists(cached)) or (\n",
    "                os.path.getmtime(cached) < os.path.getmtime(abs_path)\n",
    "            ):\n",
    "                shutil.copy2(abs_path, cached)\n",
    "            read_path = cached\n",
    "        except Exception:\n",
    "            read_path = abs_path\n",
    "    else:\n",
    "        read_path = abs_path\n",
    "\n",
    "    last_err = None\n",
    "    for eng in _def_open_backends:\n",
    "        try:\n",
    "            return xr.open_dataset(read_path, engine=eng)\n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "            continue\n",
    "    raise RuntimeError(\n",
    "        f\"Failed to open {abs_path}. Please install netCDF4 or h5netcdf, \"\n",
    "        f\"or move the data to an ASCII path. Original error: {last_err}\"\n",
    "    )\n",
    "\n",
    "# ========================== 3) Latitude and Longitude Extraction ==========================\n",
    "def _extract_scalar_value(arr) -> float:\n",
    "    try:\n",
    "        val = arr.values\n",
    "        if np.ndim(val) == 0:\n",
    "            return float(val)\n",
    "        else:\n",
    "            return float(np.nanmean(val))\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "def get_lat_lon(ds: xr.Dataset):\n",
    "    lat_candidates = ['lat', 'latitude', 'LAT', 'Latitude']\n",
    "    lon_candidates = ['lon', 'longitude', 'LON', 'Longitude']\n",
    "\n",
    "    lat, lon = np.nan, np.nan\n",
    "    for name in lat_candidates:\n",
    "        if name in ds.coords:\n",
    "            lat = _extract_scalar_value(ds.coords[name]); break\n",
    "        if name in ds.variables:\n",
    "            lat = _extract_scalar_value(ds.variables[name]); break\n",
    "    for name in lon_candidates:\n",
    "        if name in ds.coords:\n",
    "            lon = _extract_scalar_value(ds.coords[name]); break\n",
    "        if name in ds.variables:\n",
    "            lon = _extract_scalar_value(ds.variables[name]); break\n",
    "\n",
    "    if np.isnan(lat):\n",
    "        for k in ['site_latitude', 'Latitude', 'LAT', 'lat']:\n",
    "            if k in ds.attrs:\n",
    "                try: lat = float(ds.attrs[k]); break\n",
    "                except: pass\n",
    "    if np.isnan(lon):\n",
    "        for k in ['site_longitude', 'Longitude', 'LON', 'lon']:\n",
    "            if k in ds.attrs:\n",
    "                try: lon = float(ds.attrs[k]); break\n",
    "                except: pass\n",
    "    if np.isnan(lat): lat = 0.0\n",
    "    if np.isnan(lon): lon = 0.0\n",
    "    if lon > 180: lon -= 360.0\n",
    "    return float(lat), float(lon)\n",
    "\n",
    "# ========================== 4) Single-site Preprocessing ==========================\n",
    "def preprocess_site(met_path, flux_path, lat, lon, site_name):\n",
    "    try:\n",
    "        met_ds = xr_open(met_path)\n",
    "        flux_ds = xr_open(flux_path)\n",
    "\n",
    "        met_df = met_ds.to_dataframe().reset_index()\n",
    "        flux_df = flux_ds.to_dataframe().reset_index()\n",
    "\n",
    "        df = pd.merge_asof(\n",
    "            met_df.sort_values('time'),\n",
    "            flux_df.sort_values('time'),\n",
    "            on='time'\n",
    "        )\n",
    "\n",
    "        keep_cols = ['time'] + features_raw + ['GPP', 'NEE']\n",
    "        df = df[keep_cols].dropna()\n",
    "\n",
    "        # Derived features\n",
    "        df['SW_LAI'] = df['SWdown'] * df['LAI']\n",
    "        df['RH_Tair'] = df['RH'] * df['Tair']\n",
    "        df['SWdown_lag1'] = df['SWdown'].shift(1)\n",
    "        df['Tair_lag1'] = df['Tair'].shift(1)\n",
    "\n",
    "        # Site metadata\n",
    "        df['latitude']  = np.float32(lat)\n",
    "        df['longitude'] = np.float32(lon)\n",
    "        df['site'] = site_name\n",
    "\n",
    "        df = df.dropna().sort_values('time')  # Keep time column, split by site end later\n",
    "        for c in [*features_all, 'GPP', 'NEE']:\n",
    "            df[c] = df[c].astype('float32')\n",
    "\n",
    "        met_ds.close(); flux_ds.close()\n",
    "        del met_ds, flux_ds, met_df, flux_df\n",
    "        gc.collect()\n",
    "        return df\n",
    "    except KeyError as e:\n",
    "        print(f\"‚ùå Preprocessing failed (missing variables): {met_path}\\nMissing column: {e}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Preprocessing failed: {met_path}\\nError: {e}\")\n",
    "        return None\n",
    "\n",
    "# ========================== 5) Leave-out hold-out set at the end of each site ==========================\n",
    "def split_train_test_by_site(df_all: pd.DataFrame, test_frac: float = TEST_FRAC_PER_SITE):\n",
    "    train_parts, test_parts = [], []\n",
    "    for site, df_site in df_all.groupby('site'):\n",
    "        df_site = df_site.sort_values('time')\n",
    "        n = len(df_site)\n",
    "        n_test = max(1, int(round(n * test_frac)))\n",
    "        test_parts.append(df_site.iloc[-n_test:])\n",
    "        train_parts.append(df_site.iloc[:-n_test] if n_test < n else df_site.iloc[:0])\n",
    "\n",
    "    train_df = pd.concat(train_parts, axis=0).reset_index(drop=True)\n",
    "    test_df  = pd.concat(test_parts,  axis=0).reset_index(drop=True)\n",
    "    return train_df, test_df\n",
    "\n",
    "# ========================== 6) Training (FLAML AutoML) and Saving ==========================\n",
    "def train_and_save_flaml(df_all: pd.DataFrame, target: str, limit: int):\n",
    "    assert target in ['NEE', 'GPP']\n",
    "\n",
    "    # 1) Split by-site tail into hold-out test set\n",
    "    train_df, test_df = split_train_test_by_site(df_all, TEST_FRAC_PER_SITE)\n",
    "    print(f\"üß™ Split done: train={len(train_df)}, test={len(test_df)} (by-site tail {int(TEST_FRAC_PER_SITE*100)}%)\")\n",
    "\n",
    "    X_train = train_df[features_all].copy()\n",
    "    y_train = train_df[target].astype('float32').values\n",
    "    X_test  = test_df[features_all].copy()\n",
    "    y_test  = test_df[target].astype('float32').values\n",
    "    groups  = train_df['site'].values  # GroupKFold by site (handled by FLAML)\n",
    "\n",
    "    # 2) Estimator list (depends on environment availability)\n",
    "    estimator_list = []\n",
    "    try:\n",
    "        import xgboost  # noqa\n",
    "        estimator_list.append('xgboost')\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        import lightgbm  # noqa\n",
    "        estimator_list.append('lgbm')\n",
    "    except Exception:\n",
    "        pass\n",
    "    estimator_list += ['rf', 'extra_tree', 'lrl1']  # Fallback\n",
    "\n",
    "    time_budget = time_budget_map.get(limit, default_time_budget)\n",
    "\n",
    "    # 3) FLAML settings (do not pass fit_kwargs_by_estimator anymore)\n",
    "    from flaml import AutoML\n",
    "    automl = AutoML()\n",
    "    automl_settings = {\n",
    "        \"task\": \"regression\",\n",
    "        \"metric\": \"r2\",\n",
    "        \"estimator_list\": estimator_list,\n",
    "        \"log_file_name\": f\"flaml_{limit}sites_{target}.log\",\n",
    "        \"eval_method\": \"cv\",\n",
    "        \"n_splits\": N_SPLITS,\n",
    "        \"split_type\": \"group\",\n",
    "        \"groups\": groups,\n",
    "        \"time_budget\": time_budget,\n",
    "        \"seed\": RANDOM_SEED,\n",
    "        \"verbose\": 0,\n",
    "    }\n",
    "\n",
    "    print(f\"‚öôÔ∏è  FLAML fit: target={target}, sites={limit}, budget={time_budget}s, estimators={estimator_list}\")\n",
    "    automl.fit(X_train=X_train, y_train=y_train, **automl_settings)\n",
    "\n",
    "    # 4) Evaluation on hold-out set\n",
    "    y_pred = automl.predict(X_test)\n",
    "    r2   = r2_score(y_test, y_pred)\n",
    "    rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "    mae  = mean_absolute_error(y_test, y_pred)\n",
    "    try:\n",
    "        rho = pearsonr(y_test, y_pred)[0]\n",
    "    except Exception:\n",
    "        rho = np.nan\n",
    "\n",
    "    # 5) Save model\n",
    "    model_path = f'flaml_{limit}sites_{target}.pkl'\n",
    "    joblib.dump(automl, model_path)\n",
    "    print(f\"‚úÖ Saved: {model_path}\")\n",
    "\n",
    "    return {\n",
    "        \"Sites\": limit,\n",
    "        \"Target\": target,\n",
    "        \"BestEstimator\": automl.best_estimator,\n",
    "        \"BestConfig\": str(automl.best_config),\n",
    "        \"BestR2_CV\": (None if automl.best_loss is None else 1 - automl.best_loss),\n",
    "        \"R2_Holdout\": r2,\n",
    "        \"RMSE_Holdout\": rmse,\n",
    "        \"MAE_Holdout\": mae,\n",
    "        \"Rho_Holdout\": rho,\n",
    "        \"TimeBudgetSec\": time_budget\n",
    "    }\n",
    "\n",
    "# ========================== 7) Main Workflow ==========================\n",
    "def main():\n",
    "    print(f\"Current working directory: {os.getcwd()}\")\n",
    "    met_files_all = sorted(glob.glob(os.path.join(met_folder, \"*_Met.nc\")))\n",
    "    print(f\"Total {len(met_files_all)} matched Met files\")\n",
    "    if len(met_files_all) == 0:\n",
    "        raise RuntimeError(\"No *_Met.nc files found, please check met_folder path.\")\n",
    "\n",
    "    summary_rows = []\n",
    "\n",
    "    for limit in site_limits:\n",
    "        t0 = time.time()\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(f\"üöÄ Processing first {limit} sites ...\")\n",
    "\n",
    "        all_data, used_sites = [], 0\n",
    "        for met_path in met_files_all[:limit]:\n",
    "            site_prefix = os.path.basename(met_path).replace(\"_Met.nc\", \"\")\n",
    "            flux_path = os.path.join(flux_folder, site_prefix + \"_Flux.nc\")\n",
    "            if not os.path.exists(flux_path):\n",
    "                print(f\"‚ö†Ô∏è Missing Flux file, skipping {site_prefix}\")\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                ds = xr_open(met_path)\n",
    "                lat, lon = get_lat_lon(ds)\n",
    "                ds.close(); del ds\n",
    "            except Exception:\n",
    "                lat, lon = 0.0, 0.0\n",
    "\n",
    "            df_site = preprocess_site(met_path, flux_path, lat, lon, site_prefix)\n",
    "            if df_site is not None and len(df_site) > 0:\n",
    "                all_data.append(df_site)\n",
    "                used_sites += 1\n",
    "\n",
    "        if not all_data:\n",
    "            print(f\"‚ùå limit={limit} Failed to load any site data, skipped.\")\n",
    "            continue\n",
    "\n",
    "        df_all = pd.concat(all_data, axis=0).reset_index(drop=True)\n",
    "        print(f\"‚úÖ Data prepared: sites {used_sites}, total records {len(df_all)}\")\n",
    "\n",
    "        res_nee = train_and_save_flaml(df_all, 'NEE', limit)\n",
    "        res_gpp = train_and_save_flaml(df_all, 'GPP', limit)\n",
    "\n",
    "        elapsed = round(time.time() - t0, 2)\n",
    "        for r in (res_nee, res_gpp):\n",
    "            r[\"WallTimeSec_Total\"] = elapsed\n",
    "            summary_rows.append(r)\n",
    "\n",
    "        del all_data, df_all, res_nee, res_gpp\n",
    "        gc.collect()\n",
    "\n",
    "    if summary_rows:\n",
    "        df_sum = pd.DataFrame(summary_rows)\n",
    "        df_sum.to_csv(\"automl_flaml_results_summary.csv\", index=False)\n",
    "        print(\"\\nüìä Generated automl_flaml_results_summary.csv and saved all models.\")\n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è No results produced, please check data and environment.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "climate",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
