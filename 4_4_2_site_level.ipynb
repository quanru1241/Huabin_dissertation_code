{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88ed6e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working dir: /root/autodl-tmp/dataset\n",
      "\n",
      "=== Evaluating automl_10sites_GPP.pkl ===\n",
      "Transformation Pipeline and Model Successfully Loaded\n",
      "Saved: eval_3_4_2_outputs/site_metrics_10sites_GPP.csv\n",
      "Elapsed: 69.7s\n",
      "\n",
      "=== Evaluating automl_10sites_NEE.pkl ===\n",
      "Transformation Pipeline and Model Successfully Loaded\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Section 3.4.2 - Site-level error evaluation\n",
    "- Loads all saved AutoML models: automl_{5|10|20|50|170}sites_{GPP|NEE}.pkl\n",
    "- Rebuilds per-site datasets with the SAME preprocessing as training\n",
    "- Uses the last 10% per-site as test split (aligning with train_size=0.90)\n",
    "- Computes per-site metrics: R2, RMSE, MAE, Pearson r (optional MAPE/RMSLE)\n",
    "- Saves: per-model per-site CSV, overall summary CSV, RMSE boxplots for GPP/NEE\n",
    "\"\"\"\n",
    "\n",
    "import os, re, glob, sys, gc, time, shutil\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "\n",
    "# Prefer PyCaret's load_model (more stable), otherwise fall back to pickle\n",
    "try:\n",
    "    from pycaret.regression import load_model as pyc_load_model\n",
    "except Exception:\n",
    "    pyc_load_model = None\n",
    "import pickle\n",
    "\n",
    "# ========================== 0) Config (modify as needed) ==========================\n",
    "MODELS_DIR = \"./\"  # directory containing .pkl models\n",
    "MET_FOLDER = \"plumber2_met_nc_files\"\n",
    "FLUX_FOLDER = \"plumber2_nc_files\"\n",
    "\n",
    "# Features consistent with training\n",
    "FEATURES_RAW = ['SWdown', 'LWdown', 'Tair', 'Qair', 'RH', 'Psurf', 'Wind',\n",
    "                'CO2air', 'VPD', 'LAI', 'Ustar']\n",
    "DERIVED_FEATURES = ['SW_LAI', 'RH_Tair', 'SWdown_lag1', 'Tair_lag1']\n",
    "GEO_FEATURES = ['latitude', 'longitude']\n",
    "FEATURES_ALL = FEATURES_RAW + DERIVED_FEATURES + GEO_FEATURES\n",
    "\n",
    "TARGET_COLS = {\"GPP\": \"GPP\", \"NEE\": \"NEE\"}  # modify if column names differ\n",
    "SITE_COL_NAME = \"site\"  # site name column used in exported metrics\n",
    "\n",
    "# Training scales to be evaluated (detected from filenames, or restricted here)\n",
    "SCALES_ORDER = [\"5sites\", \"10sites\", \"20sites\", \"50sites\", \"170sites\"]\n",
    "\n",
    "# Output directory\n",
    "OUT_DIR = Path(\"./eval_3_4_2_outputs\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ========================== 1) xarray open & preprocessing (same as training) ==========================\n",
    "_DEF_OPEN_BACKENDS = (\"netcdf4\", \"h5netcdf\")\n",
    "\n",
    "def _has_non_ascii(s: str) -> bool:\n",
    "    try:\n",
    "        s.encode('ascii')\n",
    "        return False\n",
    "    except UnicodeEncodeError:\n",
    "        return True\n",
    "\n",
    "def _ascii_cache_path(src_path: str) -> str:\n",
    "    cache_root = Path(os.getenv('TEMP', 'C:\\\\temp')) / 'nc_cache'\n",
    "    cache_root.mkdir(parents=True, exist_ok=True)\n",
    "    return str(cache_root / Path(src_path).name)\n",
    "\n",
    "def xr_open(path):\n",
    "    abs_path = str(Path(path).resolve())\n",
    "    if sys.platform.startswith('win') and _has_non_ascii(abs_path):\n",
    "        cached = _ascii_cache_path(abs_path)\n",
    "        try:\n",
    "            if (not os.path.exists(cached)) or (os.path.getmtime(cached) < os.path.getmtime(abs_path)):\n",
    "                shutil.copy2(abs_path, cached)\n",
    "            read_path = cached\n",
    "        except Exception:\n",
    "            read_path = abs_path\n",
    "    else:\n",
    "        read_path = abs_path\n",
    "\n",
    "    last_err = None\n",
    "    for eng in _DEF_OPEN_BACKENDS:\n",
    "        try:\n",
    "            return xr.open_dataset(read_path, engine=eng)\n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "            continue\n",
    "    raise RuntimeError(f\"Failed to open {abs_path}, please install netCDF4/h5netcdf. Original error: {last_err}\")\n",
    "\n",
    "def _extract_scalar_value(arr) -> float:\n",
    "    try:\n",
    "        val = arr.values\n",
    "        if np.ndim(val) == 0:\n",
    "            return float(val)\n",
    "        else:\n",
    "            return float(np.nanmean(val))\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "def get_lat_lon(ds: xr.Dataset):\n",
    "    lat_candidates = ['lat', 'latitude', 'LAT', 'Latitude']\n",
    "    lon_candidates = ['lon', 'longitude', 'LON', 'Longitude']\n",
    "\n",
    "    lat, lon = np.nan, np.nan\n",
    "    for name in lat_candidates:\n",
    "        if name in ds.coords:  lat = _extract_scalar_value(ds.coords[name]); break\n",
    "        if name in ds.variables: lat = _extract_scalar_value(ds.variables[name]); break\n",
    "    for name in lon_candidates:\n",
    "        if name in ds.coords:  lon = _extract_scalar_value(ds.coords[name]); break\n",
    "        if name in ds.variables: lon = _extract_scalar_value(ds.variables[name]); break\n",
    "\n",
    "    if np.isnan(lat):\n",
    "        for k in ['site_latitude', 'Latitude', 'LAT', 'lat']:\n",
    "            if k in ds.attrs:\n",
    "                try: lat = float(ds.attrs[k]); break\n",
    "                except: pass\n",
    "    if np.isnan(lon):\n",
    "        for k in ['site_longitude', 'Longitude', 'LON', 'lon']:\n",
    "            if k in ds.attrs:\n",
    "                try: lon = float(ds.attrs[k]); break\n",
    "                except: pass\n",
    "    if np.isnan(lat): lat = 0.0\n",
    "    if np.isnan(lon): lon = 0.0\n",
    "    if lon > 180: lon -= 360.0\n",
    "    return float(lat), float(lon)\n",
    "\n",
    "def preprocess_site(met_path, flux_path, lat, lon):\n",
    "    \"\"\"Consistent with training, but keep time for 90/10 temporal split.\"\"\"\n",
    "    try:\n",
    "        met_ds = xr_open(met_path)\n",
    "        flux_ds = xr_open(flux_path)\n",
    "        met_df = met_ds.to_dataframe().reset_index()\n",
    "        flux_df = flux_ds.to_dataframe().reset_index()\n",
    "\n",
    "        df = pd.merge_asof(\n",
    "            met_df.sort_values('time'),\n",
    "            flux_df.sort_values('time'),\n",
    "            on='time'\n",
    "        )\n",
    "\n",
    "        keep_cols = ['time'] + FEATURES_RAW + ['GPP', 'NEE']\n",
    "        df = df[keep_cols].dropna()\n",
    "\n",
    "        df['SW_LAI'] = df['SWdown'] * df['LAI']\n",
    "        df['RH_Tair'] = df['RH'] * df['Tair']\n",
    "        df['SWdown_lag1'] = df['SWdown'].shift(1)\n",
    "        df['Tair_lag1'] = df['Tair'].shift(1)\n",
    "\n",
    "        df['latitude']  = np.float32(lat)\n",
    "        df['longitude'] = np.float32(lon)\n",
    "\n",
    "        df = df.dropna().sort_values('time').reset_index(drop=True)\n",
    "        # Not converting to float32 here to avoid dtype sensitivity in some pipelines;\n",
    "        # uncomment below if consistency is required:\n",
    "        # df[df.columns] = df[df.columns].astype('float32')\n",
    "        met_ds.close(); flux_ds.close()\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Preprocessing failed: {met_path} -> {e}\")\n",
    "        return None\n",
    "\n",
    "# ========================== 2) Model & metrics utilities ==========================\n",
    "def parse_model_filename(fname: str) -> Tuple[str, str]:\n",
    "    \"\"\"\n",
    "    Parse (scale, target) from filename, e.g.:\n",
    "      automl_20sites_GPP.pkl -> (\"20sites\", \"GPP\")\n",
    "      automl_model_5sites_NEE.pkl -> (\"5sites\", \"NEE\")\n",
    "      automl_gpu_20sites_GPP.pkl -> (\"20sites\", \"GPP\")\n",
    "    \"\"\"\n",
    "    base = os.path.basename(fname).replace(\".pkl\", \"\")\n",
    "    m = re.search(r\"(?:automl(?:_model)?(?:_gpu)?)_(\\d+sites)_(GPP|NEE)$\", base, re.IGNORECASE)\n",
    "    if not m:\n",
    "        # fallback\n",
    "        m = re.search(r\"(GPP|NEE).*?(\\d+sites)\", base, re.IGNORECASE)\n",
    "        if not m:\n",
    "            raise ValueError(f\"Cannot parse scale/target: {base}\")\n",
    "        g1, g2 = m.group(1).upper(), m.group(2).lower()\n",
    "        return (g2, g1)\n",
    "    return (m.group(1).lower(), m.group(2).upper())\n",
    "\n",
    "def load_any_model(path: str):\n",
    "    # Prefer PyCaret's load_model (accepts with or without .pkl)\n",
    "    if pyc_load_model is not None:\n",
    "        try:\n",
    "            stem = path[:-4] if path.endswith(\".pkl\") else path\n",
    "            return pyc_load_model(stem)\n",
    "        except Exception:\n",
    "            pass\n",
    "    # Fallback to native pickle\n",
    "    with open(path, \"rb\") as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "def safe_mape(y_true: np.ndarray, y_pred: np.ndarray, eps: float = 1e-6) -> float:\n",
    "    denom = np.clip(np.abs(y_true), eps, None)\n",
    "    return float(np.mean(np.abs((y_true - y_pred) / denom)) * 100.0)\n",
    "\n",
    "def safe_rmsle(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "    mask = (y_true > -1.0) & (y_pred > -1.0)\n",
    "    if mask.sum() == 0:\n",
    "        return float(\"nan\")\n",
    "    return float(np.sqrt(np.mean((np.log1p(y_pred[mask]) - np.log1p(y_true[mask])) ** 2)))\n",
    "\n",
    "def compute_metrics(y_true, y_pred, target: str) -> Dict[str, float]:\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2  = r2_score(y_true, y_pred)\n",
    "    rho = pearsonr(y_true, y_pred)[0] if (np.std(y_true)>1e-12 and np.std(y_pred)>1e-12) else np.nan\n",
    "    out = {\"MAE\": mae, \"RMSE\": rmse, \"R2\": r2, \"PearsonR\": rho}\n",
    "    # RMSLE only for GPP (since NEE may contain negative values)\n",
    "    out[\"RMSLE\"] = safe_rmsle(y_true, y_pred) if target == \"GPP\" else np.nan\n",
    "    # MAPE can explode near zero, included as reference\n",
    "    out[\"MAPE\"] = safe_mape(y_true, y_pred)\n",
    "    return out\n",
    "\n",
    "# ========================== 3) Build evaluation dataset (per site) ==========================\n",
    "def list_sites(met_folder: str) -> List[str]:\n",
    "    mets = sorted(glob.glob(os.path.join(met_folder, \"*_Met.nc\")))\n",
    "    return [os.path.basename(p).replace(\"_Met.nc\", \"\") for p in mets]\n",
    "\n",
    "def build_site_df(site_prefix: str) -> pd.DataFrame:\n",
    "    met_path  = os.path.join(MET_FOLDER,  f\"{site_prefix}_Met.nc\")\n",
    "    flux_path = os.path.join(FLUX_FOLDER, f\"{site_prefix}_Flux.nc\")\n",
    "    if not os.path.exists(met_path) or not os.path.exists(flux_path):\n",
    "        return None\n",
    "    # Extract latitude/longitude\n",
    "    try:\n",
    "        ds = xr_open(met_path)\n",
    "        lat, lon = get_lat_lon(ds)\n",
    "        ds.close()\n",
    "    except Exception:\n",
    "        lat, lon = 0.0, 0.0\n",
    "    df = preprocess_site(met_path, flux_path, lat, lon)\n",
    "    if df is None or df.empty:\n",
    "        return None\n",
    "    # Add site name\n",
    "    df[SITE_COL_NAME] = site_prefix\n",
    "    return df\n",
    "\n",
    "def per_site_time_split(df_site: pd.DataFrame, test_ratio=0.10) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"Split chronologically into 90/10, returning (train, test). Only test is used for evaluation here.\"\"\"\n",
    "    df_site = df_site.sort_values(\"time\").reset_index(drop=True)\n",
    "    n = len(df_site)\n",
    "    k = max(1, int((1.0 - test_ratio) * n))\n",
    "    return df_site.iloc[:k, :], df_site.iloc[k:, :]\n",
    "\n",
    "# ========================== 4) Main evaluation workflow ==========================\n",
    "def evaluate_model_on_all_sites(model_path: str, sites: List[str]) -> pd.DataFrame:\n",
    "    scale, target = parse_model_filename(model_path)\n",
    "    target_col = TARGET_COLS[target]\n",
    "    model = load_any_model(model_path)\n",
    "\n",
    "    records = []\n",
    "    for site in sites:\n",
    "        df_site = build_site_df(site)\n",
    "        if df_site is None or df_site.empty:\n",
    "            print(f\"[Skip] {site}: failed to build data\")\n",
    "            continue\n",
    "\n",
    "        # Temporal split: last 10% as test\n",
    "        _, test_df = per_site_time_split(df_site, test_ratio=0.10)\n",
    "        if test_df.empty:\n",
    "            continue\n",
    "\n",
    "        # Features & target\n",
    "        X_test = test_df[FEATURES_ALL].copy()\n",
    "        y_test = test_df[target_col].values\n",
    "\n",
    "        # Prediction\n",
    "        try:\n",
    "            y_pred = model.predict(X_test)\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] Prediction failed {os.path.basename(model_path)} @ {site}: {e}\")\n",
    "            continue\n",
    "\n",
    "        metrics = compute_metrics(y_test, y_pred, target)\n",
    "        rec = {\n",
    "            \"model_file\": os.path.basename(model_path),\n",
    "            \"scale\": scale, \"target\": target, \"site\": site,\n",
    "            \"lat\": float(test_df[\"latitude\"].iloc[0]),\n",
    "            \"lon\": float(test_df[\"longitude\"].iloc[0])\n",
    "        }\n",
    "        rec.update(metrics)\n",
    "        records.append(rec)\n",
    "\n",
    "    df_res = pd.DataFrame(records)\n",
    "    if not df_res.empty:\n",
    "        out_csv = OUT_DIR / f\"site_metrics_{scale}_{target}.csv\"\n",
    "        df_res.to_csv(out_csv, index=False)\n",
    "        print(f\"Saved: {out_csv}\")\n",
    "    return df_res\n",
    "\n",
    "def plot_rmse_boxplots(all_df: pd.DataFrame, target: str):\n",
    "    df_t = all_df[all_df[\"target\"] == target].copy()\n",
    "    if df_t.empty:\n",
    "        return\n",
    "    order = [s for s in SCALES_ORDER if s in df_t[\"scale\"].unique()]\n",
    "    df_t[\"scale\"] = pd.Categorical(df_t[\"scale\"], categories=order, ordered=True)\n",
    "    plt.figure(figsize=(7, 4.5))\n",
    "    df_t.boxplot(column=\"RMSE\", by=\"scale\", grid=False)\n",
    "    plt.title(f\"Per-site RMSE across training scales ({target})\")\n",
    "    plt.suptitle(\"\")\n",
    "    plt.xlabel(\"Training scale\")\n",
    "    plt.ylabel(\"RMSE\")\n",
    "    out_png = OUT_DIR / f\"boxplot_rmse_{target}.png\"\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_png, dpi=200)\n",
    "    plt.close()\n",
    "    print(f\"Saved: {out_png}\")\n",
    "\n",
    "def main():\n",
    "    print(f\"Working dir: {os.getcwd()}\")\n",
    "    model_paths = sorted(glob.glob(os.path.join(MODELS_DIR, \"*.pkl\")))\n",
    "    keep = []\n",
    "    for p in model_paths:\n",
    "        try:\n",
    "            parse_model_filename(p)\n",
    "            keep.append(p)\n",
    "        except Exception:\n",
    "            pass\n",
    "    if not keep:\n",
    "        raise RuntimeError(\"No parsable .pkl models found. Please ensure filenames like automl_20sites_GPP.pkl\")\n",
    "\n",
    "    # List available sites\n",
    "    sites = list_sites(MET_FOLDER)\n",
    "    if not sites:\n",
    "        raise RuntimeError(\"No *_Met.nc found, please check MET_FOLDER path.\")\n",
    "\n",
    "    all_list = []\n",
    "    for mp in keep:\n",
    "        print(f\"\\n=== Evaluating {os.path.basename(mp)} ===\")\n",
    "        t0 = time.time()\n",
    "        df_res = evaluate_model_on_all_sites(mp, sites)\n",
    "        if df_res is not None and not df_res.empty:\n",
    "            all_list.append(df_res)\n",
    "        print(f\"Elapsed: {time.time()-t0:.1f}s\")\n",
    "\n",
    "    if not all_list:\n",
    "        print(\"No results produced.\")\n",
    "        return\n",
    "\n",
    "    all_df = pd.concat(all_list, ignore_index=True)\n",
    "    all_df.to_csv(OUT_DIR / \"all_site_metrics_all_models.csv\", index=False)\n",
    "    print(f\"Saved: {OUT_DIR/'all_site_metrics_all_models.csv'}\")\n",
    "\n",
    "    # Summary: by (target, scale)\n",
    "    summary = (all_df.groupby([\"target\", \"scale\"])\n",
    "                      .agg(n_sites=(\"site\",\"nunique\"),\n",
    "                           MAE_mean=(\"MAE\",\"mean\"),\n",
    "                           RMSE_mean=(\"RMSE\",\"mean\"),\n",
    "                           R2_mean=(\"R2\",\"mean\"),\n",
    "                           PearsonR_mean=(\"PearsonR\",\"mean\"))\n",
    "                      .reset_index())\n",
    "    summary.to_csv(OUT_DIR / \"summary_by_scale_target.csv\", index=False)\n",
    "    print(f\"Saved: {OUT_DIR/'summary_by_scale_target.csv'}\")\n",
    "\n",
    "    # Boxplots (for Section 3.4.2)\n",
    "    plot_rmse_boxplots(all_df, \"GPP\")\n",
    "    plot_rmse_boxplots(all_df, \"NEE\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "climate",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
